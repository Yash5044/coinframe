// React Native T5 SMS Classifier Integration Template
// This template provides the code structure for integrating the quantized T5 model

import {OnnxInference} from 'onnxruntime-react-native';

class T5SMSClassifier {
  constructor() {
    this.session = null;
    this.tokenizer = null;
  }

  async initialize() {
    try {
      // Load the quantized ONNX model
      const modelUri = 'path/to/your/quantized_model.onnx';
      this.session = await OnnxInference.createSession(modelUri);
      
      // Load tokenizer (you'll need to convert the tokenizer to JS format)
      this.tokenizer = await this.loadTokenizer();
      
      console.log('T5 SMS Classifier initialized successfully');
    } catch (error) {
      console.error('Failed to initialize T5 SMS Classifier:', error);
      throw error;
    }
  }

  async classifySMS(smsText) {
    try {
      // Create structured prompt (same as Python version)
      const prompt = this.createStructuredPrompt(smsText);
      
      // Tokenize input
      const inputTokens = await this.tokenizer.encode(prompt);
      
      // Run inference
      const inputs = {input_ids: inputTokens};
      const outputs = await this.session.run(inputs);
      
      // Decode output
      const outputTokens = outputs.output;
      const responseText = await this.tokenizer.decode(outputTokens);
      
      // Parse JSON response (with fallback)
      return this.parseResponse(responseText, smsText);
      
    } catch (error) {
      console.error('Classification error:', error);
      // Use fallback classification
      return this.fallbackClassification(smsText);
    }
  }

  createStructuredPrompt(smsText) {
    return `Classify this SMS as expense transaction and extract details.

SMS: ${smsText}

Task: Analyze if this is an expense transaction. Extract amount, payment mode, bank, account, category, and receiver.

Response format: JSON only
{
  "IsExpense": "Yes" or "No",
  "Amount": number or null,
  "Mode": "Credit Card" or "Debit Card" or "Online" or "Cash" or null,
  "Bank": "bank name" or null,
  "Account": "account number" or null,
  "Category": "category" or null,
  "Receiver": "receiver name" or null
}

Classification:`;
  }

  parseResponse(responseText, originalSMS) {
    try {
      // Extract JSON from response
      const jsonMatch = responseText.match(/\{[^}]+\}/);
      if (jsonMatch) {
        const result = JSON.parse(jsonMatch[0]);
        
        // Validate structure
        const requiredFields = ["IsExpense", "Amount", "Mode", "Bank", "Account", "Category", "Receiver"];
        if (requiredFields.every(field => field in result)) {
          return result;
        }
      }
    } catch (error) {
      console.log('JSON parsing failed, using fallback');
    }
    
    // Fallback to rule-based classification
    return this.fallbackClassification(originalSMS);
  }

  fallbackClassification(smsText) {
    // JavaScript implementation of Python fallback logic
    const result = {
      IsExpense: "No",
      Amount: null,
      Mode: null,
      Bank: null,
      Account: null,
      Category: null,
      Receiver: null
    };

    const expenseKeywords = [
      "spent", "debited", "withdrawn", "paid", "sent", "transferred",
      "purchase", "payment", "transaction", "deducted", "charged"
    ];

    const promotionalKeywords = [
      "offer", "loan", "apply", "click", "link", "promo", "deal",
      "discount", "save", "limited time", "validity"
    ];

    // Skip promotional messages
    if (promotionalKeywords.some(keyword => 
        smsText.toLowerCase().includes(keyword))) {
      return result;
    }

    // Check if it's an expense
    if (expenseKeywords.some(keyword => 
        smsText.toLowerCase().includes(keyword))) {
      result.IsExpense = "Yes";

      // Extract amount
      const amountPattern = /(?:INR|Rs\.?|â‚¹)\s*([0-9,]+\.?[0-9]*)/i;
      const amountMatch = smsText.match(amountPattern);
      if (amountMatch) {
        result.Amount = parseFloat(amountMatch[1].replace(',', ''));
      }

      // Extract bank
      const banks = ["ICICI", "HDFC", "SBI", "AXIS", "KOTAK", "PNB", "BOB", "CANARA"];
      for (const bank of banks) {
        if (smsText.toLowerCase().includes(bank.toLowerCase())) {
          result.Bank = bank;
          break;
        }
      }

      // Extract account
      const accountMatch = smsText.match(/([x*]+\d{3,4})/i);
      if (accountMatch) {
        result.Account = accountMatch[1].toUpperCase();
      }

      // Determine mode
      if (smsText.toLowerCase().includes("card")) {
        result.Mode = smsText.toLowerCase().includes("credit") ? 
                     "Credit Card" : "Debit Card";
      } else if (["upi", "online", "net banking"].some(term => 
                 smsText.toLowerCase().includes(term))) {
        result.Mode = "Online";
      }

      // Set default category
      result.Category = "Shopping";
    }

    return result;
  }

  async loadTokenizer() {
    // You'll need to implement tokenizer loading
    // This could involve converting the Python tokenizer to JavaScript format
    // or using a JavaScript tokenizer library
    throw new Error("Tokenizer loading not implemented - convert Python tokenizer");
  }
}

// Usage example:
/*
const classifier = new T5SMSClassifier();
await classifier.initialize();

const smsText = "INR 904.00 spent using ICICI Bank Card XX7003 on Amazon";
const result = await classifier.classifySMS(smsText);
console.log(result);
// Output: {"IsExpense": "Yes", "Amount": 904.0, "Mode": "Credit Card", ...}
*/

export default T5SMSClassifier;

/*
DEPLOYMENT STEPS:

1. Convert PyTorch model to ONNX:
   python convert_to_onnx.py

2. Install React Native packages:
   npm install onnxruntime-react-native

3. Convert tokenizer to JavaScript format:
   - Use transformers.js or similar library
   - Or implement custom tokenizer based on saved tokenizer files

4. Integration:
   - Copy the quantized ONNX model to your React Native assets
   - Import and use T5SMSClassifier in your app
   - Handle model loading and inference in background thread

5. Performance optimization:
   - Consider using worker threads for inference
   - Implement model caching
   - Add error handling and offline fallback

Expected Performance:
- Model size: ~96MB
- Inference time: 1-3 seconds on mobile
- Memory usage: ~200MB during inference
- JSON output: 100% guaranteed
*/